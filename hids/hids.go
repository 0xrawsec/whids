package hids

import (
	"compress/gzip"
	"context"
	"fmt"
	"io/ioutil"
	"os"
	"os/exec"
	"path/filepath"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/0xrawsec/golang-etw/etw"
	"github.com/0xrawsec/golang-win32/win32"
	"github.com/0xrawsec/golang-win32/win32/kernel32"

	"github.com/0xrawsec/gene/v2/engine"
	"github.com/0xrawsec/golang-utils/crypto/data"
	"github.com/0xrawsec/golang-utils/datastructs"
	"github.com/0xrawsec/golang-utils/fsutil"
	"github.com/0xrawsec/golang-utils/fsutil/fswalker"
	"github.com/0xrawsec/golang-utils/log"
	"github.com/0xrawsec/golang-utils/sync/semaphore"
	"github.com/0xrawsec/whids/api"
	"github.com/0xrawsec/whids/event"
	"github.com/0xrawsec/whids/utils"
)

const (

	/** Private const **/

	// Container extension
	containerExt = ".cont.gz"
)

var (
	/** Public vars **/

	ContainRuleName = "EDR containment"
	MaxEPS          = float64(300)
	MaxEPSDuration  = 30 * time.Second

	/** Private vars **/

	emptyForwarderConfig = api.ForwarderConfig{}

	// extensions of files to upload to manager
	uploadExts = datastructs.NewInitSyncedSet(".gz", ".sha256")

	archivedRe = regexp.MustCompile(`(CLIP-)??[0-9A-F]{32,}(\..*)?`)
)

// HIDS structure
type HIDS struct {
	sync.RWMutex // Mutex to lock the IDS when updating rules
	ctx          context.Context
	cancel       context.CancelFunc

	eventProvider   *etw.Consumer
	stats           *EventStats
	preHooks        *HookManager
	postHooks       *HookManager
	forwarder       *api.Forwarder
	channels        *datastructs.SyncedSet // Windows log channels to listen to
	channelsSignals chan bool
	config          *Config
	//eventScanned    uint64
	//alertReported   uint64
	//startTime time.Time
	waitGroup sync.WaitGroup

	flagProcTermEn bool
	bootCompleted  bool
	// Sysmon GUID of HIDS process
	guid          string
	tracker       *ActivityTracker
	actionHandler *ActionHandler
	memdumped     *datastructs.SyncedSet
	dumping       *datastructs.SyncedSet
	filedumped    *datastructs.SyncedSet
	hookSemaphore semaphore.Semaphore

	Engine   *engine.Engine
	DryRun   bool
	PrintAll bool
}

func newActionnableEngine(c *Config) (e *engine.Engine) {
	e = engine.NewEngine()
	e.ShowActions = true
	if c.Actions.Low != nil {
		e.SetDefaultActions(actionLowLow, actionLowHigh, c.Actions.Low)
	}
	if c.Actions.Medium != nil {
		e.SetDefaultActions(actionMediumLow, actionMediumHigh, c.Actions.Medium)
	}
	if c.Actions.High != nil {
		e.SetDefaultActions(actionHighLow, actionHighHigh, c.Actions.High)
	}
	if c.Actions.Critical != nil {
		e.SetDefaultActions(actionCriticalLow, actionCriticalHigh, c.Actions.Critical)
	}
	return
}

// NewHIDS creates a new HIDS object from configuration
func NewHIDS(c *Config) (h *HIDS, err error) {

	ctx, cancel := context.WithCancel(context.Background())

	h = &HIDS{
		ctx:             ctx,
		cancel:          cancel,
		eventProvider:   etw.NewRealTimeConsumer(ctx),
		stats:           NewEventStats(MaxEPS, MaxEPSDuration),
		preHooks:        NewHookMan(),
		postHooks:       NewHookMan(),
		channels:        datastructs.NewSyncedSet(),
		channelsSignals: make(chan bool),
		config:          c,
		waitGroup:       sync.WaitGroup{},
		tracker:         NewActivityTracker(),
		memdumped:       datastructs.NewSyncedSet(),
		dumping:         datastructs.NewSyncedSet(),
		filedumped:      datastructs.NewSyncedSet(),
		hookSemaphore:   semaphore.New(4),
	}

	// initializing action manager
	h.actionHandler = NewActionHandler(h)

	// Creates missing directories
	c.Prepare()

	// Create logfile asap if needed
	if c.Logfile != "" {
		log.SetLogfile(c.Logfile, 0600)
	}

	// Verify configuration
	if err = c.Verify(); err != nil {
		return nil, err
	}

	// loading forwarder config
	if h.forwarder, err = api.NewForwarder(c.FwdConfig); err != nil {
		return nil, err
	}

	// cleaning up previous runs
	h.cleanup()

	// initialization
	h.initEventProvider()
	h.initHooks(c.EnableHooks)
	// initializing canaries
	h.config.CanariesConfig.Configure()
	// fixing local audit policies if necessary
	h.config.AuditConfig.Configure()

	// update and load engine
	if err := h.update(true); err != nil {
		return h, err
	}

	return h, nil
}

/** Private Methods **/

func (h *HIDS) initEventProvider() {

	// parses the providers and init filters
	for _, sprov := range h.config.EtwConfig.UnifiedProviders() {
		if prov, err := etw.ProviderFromString(sprov); err != nil {
			log.Errorf("Error while parsing provider %s: %s", sprov, err)
		} else {
			h.eventProvider.Filter.FromProvider(&prov)
		}
	}

	// open traces
	for _, trace := range h.config.EtwConfig.UnifiedTraces() {
		if err := h.eventProvider.OpenTrace(trace); err != nil {
			log.Errorf("Failed to open trace %s: %s", trace, err)
		}
	}
}

func (h *HIDS) initHooks(advanced bool) {
	// We enable those hooks anyway since it is needed to skip
	// events generated by WHIDS process. These ar very light hooks
	h.preHooks.Hook(hookSelfGUID, fltImageSize)
	h.preHooks.Hook(hookProcTerm, fltProcTermination)
	h.preHooks.Hook(hookStats, fltStats)
	h.preHooks.Hook(hookTrack, fltTrack)
	if advanced {
		// Process terminator hook, terminating blacklisted (by action) processes
		h.preHooks.Hook(hookTerminator, fltProcessCreate)
		h.preHooks.Hook(hookImageLoad, fltImageLoad)
		h.preHooks.Hook(hookSetImageSize, fltImageSize)
		h.preHooks.Hook(hookProcessIntegrityProcTamp, fltImageTampering)
		h.preHooks.Hook(hookEnrichServices, fltAnySysmon)
		h.preHooks.Hook(hookClipboardEvents, fltClipboard)
		h.preHooks.Hook(hookFileSystemAudit, fltFSObjectAccess)
		// Must be run the last as it depends on other filters
		h.preHooks.Hook(hookEnrichAnySysmon, fltAnySysmon)
		h.preHooks.Hook(hookKernelFiles, fltKernelFile)

		// This hook must run before action handling as we want
		// the gene score to be set before an eventual reporting
		h.postHooks.Hook(hookUpdateGeneScore, fltAnyEvent)
	}
}

func (h *HIDS) update(force bool) (last error) {
	var reloadRules, reloadContainers bool

	// check that we are connected to any manager
	if h.config.IsForwardingEnabled() {
		reloadRules = h.needsRulesUpdate()
		reloadContainers = h.needsIoCsUpdate()
	}

	// check if we need rule update
	if reloadRules {
		log.Info("Updating WHIDS rules")
		if err := h.fetchRulesFromManager(); err != nil {
			log.Errorf("Failed to fetch rules from manager: %s", err)
			reloadRules = false
		}
	}

	if reloadContainers {
		log.Info("Updating WHIDS containers")
		if err := h.fetchIoCsFromManager(); err != nil {
			log.Errorf("Failed to fetch containers from manager: %s", err)
			reloadContainers = false
		}
	}

	log.Debugf("reloading rules:%t containers:%t forced:%t", reloadRules, reloadContainers, force)
	if reloadRules || reloadContainers || force {
		// We need to create a new engine if we received a rule/containers update
		newEngine := newActionnableEngine(h.config)

		// containers must be loaded before the rules anyway
		log.Infof("Loading HIDS containers (used in rules) from: %s", h.config.RulesConfig.ContainersDB)
		if err := h.loadContainers(newEngine); err != nil {
			err = fmt.Errorf("failed at loading containers: %s", err)
			last = err
		}

		// Loading IOC container rules
		for _, rule := range IoCRules {
			if cr, err := rule.Compile(newEngine); err != nil {
				err = fmt.Errorf("failed to compile IOC rule: %s", err)
				last = err
			} else {
				if err = newEngine.AddRule(cr); err != nil {
					last = err
				}
			}
		}

		// Loading canary rules
		if h.config.CanariesConfig.Enable {
			log.Infof("Loading canary rules")
			// Sysmon rule
			sr := h.config.CanariesConfig.GenRuleSysmon()
			if scr, err := sr.Compile(nil); err != nil {
				err = fmt.Errorf("failed to compile canary rule: %s", err)
				last = err
			} else {
				if err = newEngine.AddRule(scr); err != nil {
					last = err
				}
			}

			// File System Audit Rule
			fsr := h.config.CanariesConfig.GenRuleFSAudit()
			if fscr, err := fsr.Compile(nil); err != nil {
				log.Errorf("Failed to compile canary rule: %s", err)
			} else {
				if err = newEngine.AddRule(fscr); err != nil {
					last = err
				}
			}

			// File System Audit Rule
			kfr := h.config.CanariesConfig.GenRuleKernelFile()
			if kfcr, err := kfr.Compile(nil); err != nil {
				log.Errorf("Failed to compile canary rule: %s", err)
			} else {
				if err = newEngine.AddRule(kfcr); err != nil {
					last = err
				}
			}
		}

		// Loading rules
		log.Infof("Loading HIDS rules from: %s", h.config.RulesConfig.RulesDB)
		if err := newEngine.LoadDirectory(h.config.RulesConfig.RulesDB); err != nil {
			last = fmt.Errorf("failed to load rules: %s", err)
		}
		log.Infof("Number of rules loaded in engine: %d", newEngine.Count())

		// updating engine if no error
		if last == nil {
			// we update engine only if there was no error
			// no need to lock HIDS as newEngine is ready to use at this point
			h.Engine = newEngine
		} else {
			log.Error("EDR engine not updated:", last)
		}
	} else {
		log.Debug("Neither rules nor containers need to be updated")
	}

	return
}

// rules needs to be updated with the new ones available in manager
func (h *HIDS) needsRulesUpdate() bool {
	var err error
	var oldSha256, sha256 string
	_, rulesSha256Path := h.config.RulesConfig.RulesPaths()

	// Don't need update if not connected to a manager
	if h.config.IsForwardingEnabled() {
		return false
	}

	if sha256, err = h.forwarder.Client.GetRulesSha256(); err != nil {
		return false
	}
	oldSha256, _ = utils.ReadFileString(rulesSha256Path)

	log.Debugf("Rules: remote=%s local=%s", sha256, oldSha256)
	return oldSha256 != sha256
}

// returns true if a container needs to be updated
func (h *HIDS) needsIoCsUpdate() bool {
	var localSha256, remoteSha256 string

	// Don't need update if not connected to a manager
	if h.config.IsForwardingEnabled() {
		return false
	}

	container := api.IoCContainerName
	_, locContSha256Path := h.containerPaths(container)

	// means that remoteCont is also a local container
	remoteSha256, _ = h.forwarder.Client.GetIoCsSha256()
	localSha256, _ = utils.ReadFileString(locContSha256Path)
	log.Infof("container %s: remote=%s local=%s", container, remoteSha256, localSha256)
	return localSha256 != remoteSha256
}

func (h *HIDS) fetchRulesFromManager() (err error) {
	var rules, sha256 string

	rulePath, sha256Path := h.config.RulesConfig.RulesPaths()

	// if we are not connected to a manager we return
	if h.config.FwdConfig.Local {
		return
	}

	log.Infof("Fetching new rules available in manager")
	if sha256, err = h.forwarder.Client.GetRulesSha256(); err != nil {
		return err
	}

	if rules, err = h.forwarder.Client.GetRules(); err != nil {
		return err
	}

	if sha256 != data.Sha256([]byte(rules)) {
		return fmt.Errorf("failed to verify rules integrity")
	}

	ioutil.WriteFile(sha256Path, []byte(sha256), 0600)
	return ioutil.WriteFile(rulePath, []byte(rules), 0600)
}

// containerPaths returns the path to the container and the path to its sha256 file
func (h *HIDS) containerPaths(container string) (path, sha256Path string) {
	path = filepath.Join(h.config.RulesConfig.ContainersDB, fmt.Sprintf("%s%s", container, containerExt))
	sha256Path = fmt.Sprintf("%s.sha256", path)
	return
}

func (h *HIDS) fetchIoCsFromManager() (err error) {
	var iocs []string
	cl := h.forwarder.Client

	// if we are not connected to a manager we return
	if h.config.FwdConfig.Local {
		return
	}

	if iocs, err = cl.GetIoCs(); err != nil {
		return
	}

	// we compare the integrity of the container received
	compSha256 := utils.Sha256StringArray(iocs)

	if sha256, err := cl.GetIoCsSha256(); err != nil {
		return fmt.Errorf("failed to get IoCs sha256: %s", err)
	} else if compSha256 != sha256 {
		return fmt.Errorf("failed to verify container \"%s\" integrity", api.IoCContainerName)
	}

	// we dump the container
	contPath, contSha256Path := h.containerPaths(api.IoCContainerName)
	fd, err := utils.HidsCreateFile(contPath)
	if err != nil {
		return err
	}
	// closing underlying file
	defer fd.Close()

	w := gzip.NewWriter(fd)
	// closing gzip writer
	defer w.Close()
	for _, ioc := range iocs {
		if _, err = w.Write([]byte(fmt.Sprintln(ioc))); err != nil {
			return
		}
	}

	if err = w.Close(); err != nil {
		return
	}

	if err = fd.Close(); err != nil {
		return
	}

	// Dump current container sha256 to a file
	return ioutil.WriteFile(contSha256Path, []byte(compSha256), 0600)
}

// loads containers found in container database directory
func (h *HIDS) loadContainers(engine *engine.Engine) (lastErr error) {
	for wi := range fswalker.Walk(h.config.RulesConfig.ContainersDB) {
		for _, fi := range wi.Files {
			path := filepath.Join(wi.Dirpath, fi.Name())
			// we take only files with good extension
			if strings.HasSuffix(fi.Name(), containerExt) {
				cont := strings.SplitN(fi.Name(), ".", 2)[0]
				fd, err := os.Open(path)
				if err != nil {
					lastErr = err
					continue
				}
				r, err := gzip.NewReader(fd)
				if err != nil {
					lastErr = err
					// we close file descriptor
					fd.Close()
					continue
				}
				log.Infof("Loading container %s from path %s", cont, path)
				if err = engine.LoadContainer(cont, r); err != nil {
					lastErr = fmt.Errorf("failed to load container %s: %s", cont, err)
					log.Error(lastErr)
				}
				r.Close()
				fd.Close()
			}
		}
	}
	return
}

func (h *HIDS) cleanup() {
	// Cleaning up empty dump directories if needed
	fis, _ := ioutil.ReadDir(h.config.Dump.Dir)
	for _, fi := range fis {
		if fi.IsDir() {
			fp := filepath.Join(h.config.Dump.Dir, fi.Name())
			if utils.CountFiles(fp) == 0 {
				os.RemoveAll(fp)
			}
		}
	}
}

////////////////// Routines

// schedules the different routines to be ran
func (h *HIDS) cronRoutine() {
	now := time.Now()
	// timestamps
	lastUpdateTs := now
	lastUploadTs := now
	lastArchDelTs := now
	lastCmdRunTs := now

	go func() {
		for {
			now = time.Now()
			switch {
			// handle updates
			case now.Sub(lastUpdateTs) >= h.config.RulesConfig.UpdateInterval:
				// put here function to update
				lastUpdateTs = now
			// handle uploads
			case now.Sub(lastUploadTs) >= time.Minute:
				lastUploadTs = now
			// handle sysmon archive cleaning
			case now.Sub(lastArchDelTs) >= time.Minute:
				// put here code to delete archived files
				lastArchDelTs = now
			// handle command to run
			case now.Sub(lastCmdRunTs) >= 5*time.Second:
				// put here code to run commands
				lastCmdRunTs = now
			}

			time.Sleep(1 * time.Second)
		}
	}()
}

func (h *HIDS) cleanArchivedRoutine() bool {
	if h.config.Sysmon.CleanArchived {
		go func() {
			log.Info("Starting routine to cleanup Sysmon archived files")
			archivePath := h.config.Sysmon.ArchiveDirectory

			if archivePath == "" {
				log.Error("Sysmon archive directory not found")
				return
			}

			if fsutil.IsDir(archivePath) {
				// used to mark files for which we already reported errors
				reported := datastructs.NewSyncedSet()
				log.Infof("Starting archive cleanup loop for directory: %s", archivePath)
				for {
					// expiration fixed to five minutes
					expired := time.Now().Add(time.Minute * -5)
					for wi := range fswalker.Walk(archivePath) {
						for _, fi := range wi.Files {
							if archivedRe.MatchString(fi.Name()) {
								path := filepath.Join(wi.Dirpath, fi.Name())
								if fi.ModTime().Before(expired) {
									// we print out error only once
									if err := os.Remove(path); err != nil && !reported.Contains(path) {
										log.Errorf("Failed to remove archived file: %s", err)
										reported.Add(path)
									}
								}
							}
						}
					}
					time.Sleep(time.Minute * 1)
				}
			} else {
				log.Errorf(fmt.Sprintf("No such Sysmon archive directory: %s", archivePath))
			}
		}()
		return true
	}
	return false
}

// returns true if the update routine is started
func (h *HIDS) updateRoutine() bool {
	d := h.config.RulesConfig.UpdateInterval
	if h.config.IsForwardingEnabled() {
		if d > 0 {
			go func() {
				t := time.NewTimer(d)
				for range t.C {
					if err := h.update(false); err != nil {
						log.Error(err)
					}
					t.Reset(d)
				}
			}()
			return true
		}
	}
	return false
}

func (h *HIDS) uploadRoutine() bool {
	if h.config.IsForwardingEnabled() {
		// force compression in this case
		h.config.Dump.Compression = true
		go func() {
			for {
				// Sending dump files over to the manager
				for wi := range fswalker.Walk(h.config.Dump.Dir) {
					for _, fi := range wi.Files {
						sp := strings.Split(wi.Dirpath, string(os.PathSeparator))
						// upload only file with some extensions
						if uploadExts.Contains(filepath.Ext(fi.Name())) {
							if len(sp) >= 2 {
								var shrink *api.UploadShrinker
								var err error

								guid := sp[len(sp)-2]
								ehash := sp[len(sp)-1]
								fullpath := filepath.Join(wi.Dirpath, fi.Name())

								// we create upload shrinker object
								if shrink, err = api.NewUploadShrinker(fullpath, guid, ehash); err != nil {
									log.Errorf("Failed to create upload iterator: %s", err)
									continue
								}

								if shrink.Size() > h.config.FwdConfig.Client.MaxUploadSize {
									log.Warnf("Dump file is above allowed upload limit, %s will be deleted without being sent", fullpath)
									goto CleanShrinker
								}

								// we shrink a file into several chunks to reduce memory impact
								for fu := shrink.Next(); fu != nil; fu = shrink.Next() {
									if err = h.forwarder.Client.PostDump(fu); err != nil {
										log.Error(err)
										break
									}
								}

							CleanShrinker:
								// close shrinker otherwise we cannot remove files
								shrink.Close()

								if shrink.Err() == nil {
									log.Infof("Dump file successfully sent to manager, deleting: %s (err=%s)", fullpath, os.Remove(fullpath))
								} else {
									log.Errorf("Failed to post dump file: %s", shrink.Err())
								}

							} else {
								log.Errorf("Unexpected directory layout, cannot send dump to manager")
							}
						}
					}

				}
				time.Sleep(60 * time.Second)
			}
		}()
		return true
	}
	return false
}

func (h *HIDS) containCmd() *exec.Cmd {
	ip := h.forwarder.Client.ManagerIP
	// only allow connection to the manager configured
	return exec.Command("netsh.exe",
		"advfirewall",
		"firewall",
		"add",
		"rule",
		fmt.Sprintf("name=%s", ContainRuleName),
		"dir=out",
		fmt.Sprintf("remoteip=0.0.0.0-%s,%s-255.255.255.255", utils.PrevIP(ip), utils.NextIP(ip)),
		"action=block")
}

func (h *HIDS) uncontainCmd() *exec.Cmd {
	return exec.Command("netsh.exe", "advfirewall",
		"firewall",
		"delete",
		"rule",
		fmt.Sprintf("name=%s", ContainRuleName),
	)
}

func (h *HIDS) handleManagerCommand(cmd *api.Command) {

	// Switch processing the commands
	switch cmd.Name {
	// Aliases
	case "contain":
		cmd.FromExecCmd(h.containCmd())
	case "uncontain":
		cmd.FromExecCmd(h.uncontainCmd())
	case "osquery":
		osquery := h.config.Report.OSQuery.Bin
		switch {
		case fsutil.IsFile(h.config.Report.OSQuery.Bin):
			cmd.Name = h.config.Report.OSQuery.Bin
			cmd.Args = append([]string{"--json", "-A"}, cmd.Args...)
			cmd.ExpectJSON = true
		case osquery == "":
			cmd.Unrunnable()
			cmd.Error = "OSQuery binary file configured does not exist"
		default:
			cmd.Unrunnable()
			cmd.Error = fmt.Sprintf("OSQuery binary file configured does not exist: %s", osquery)
		}

	// internal commands
	case "terminate":
		cmd.Unrunnable()
		if len(cmd.Args) > 0 {
			spid := cmd.Args[0]
			if pid, err := strconv.Atoi(spid); err != nil {
				cmd.Error = fmt.Sprintf("failed to parse pid: %s", err)
			} else if err := terminate(pid); err != nil {
				cmd.Error = err.Error()
			}
		}
	case "hash":
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		if len(cmd.Args) > 0 {
			if out, err := cmdHash(cmd.Args[0]); err != nil {
				cmd.Error = err.Error()
			} else {
				cmd.Json = out
			}
		}
	case "stat":
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		if len(cmd.Args) > 0 {
			if out, err := cmdStat(cmd.Args[0]); err != nil {
				cmd.Error = err.Error()
			} else {
				cmd.Json = out
			}
		}
	case "dir":
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		if len(cmd.Args) > 0 {
			if out, err := cmdDir(cmd.Args[0]); err != nil {
				cmd.Error = err.Error()
			} else {
				cmd.Json = out
			}
		}
	case "walk":
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		if len(cmd.Args) > 0 {
			cmd.Json = cmdWalk(cmd.Args[0])
		}
	case "find":
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		if len(cmd.Args) == 2 {
			if out, err := cmdFind(cmd.Args[0], cmd.Args[1]); err != nil {
				cmd.Error = err.Error()
			} else {
				cmd.Json = out
			}
		}
	case "report":
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		cmd.Json = h.Report(false)
	case "processes":
		h.tracker.RLock()
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		cmd.Json = h.tracker.PS()
		h.tracker.RUnlock()
	case "modules":
		h.tracker.RLock()
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		cmd.Json = h.tracker.Modules()
		h.tracker.RUnlock()
	case "drivers":
		h.tracker.RLock()
		cmd.Unrunnable()
		cmd.ExpectJSON = true
		cmd.Json = h.tracker.Drivers
		h.tracker.RUnlock()
	}

	// we finally run the command
	if err := cmd.Run(); err != nil {
		log.Errorf("failed to run command sent by manager \"%s\": %s", cmd.String(), err)
	}
}

// routine which manages command to be executed on the endpoint
// it is made in such a way that we can send burst of commands
func (h *HIDS) commandRunnerRoutine() bool {
	if h.config.IsForwardingEnabled() {
		go func() {

			defaultSleep := time.Second * 5
			sleep := defaultSleep

			burstDur := time.Duration(0)
			tgtBurstDur := time.Second * 30
			burstSleep := time.Millisecond * 500

			for {
				if cmd, err := h.forwarder.Client.FetchCommand(); err != nil && err != api.ErrNothingToDo {
					log.Error(err)
				} else if err == nil {
					// reduce sleeping time if a command was received
					sleep = burstSleep
					burstDur = 0
					log.Infof("Handling manager command: %s", cmd.String())
					h.handleManagerCommand(cmd)
					if err := h.forwarder.Client.PostCommand(cmd); err != nil {
						log.Error(err)
					}
				}

				// if we reached the targetted burst duration
				if burstDur >= tgtBurstDur {
					sleep = defaultSleep
				}

				if sleep == burstSleep {
					burstDur += sleep
				}

				time.Sleep(sleep)
			}
		}()
		return true
	}
	return false
}

/** Public Methods **/

// IsHIDSEvent returns true if the event is generated by IDS activity
func (h *HIDS) IsHIDSEvent(e *event.EdrEvent) bool {
	if pguid, ok := e.GetString(pathSysmonParentProcessGUID); ok {
		if pguid == h.guid {
			return true
		}
	}

	if guid, ok := e.GetString(pathSysmonProcessGUID); ok {
		if guid == h.guid {
			return true
		}
		// search for parent in processTracker
		if pt := h.tracker.GetByGuid(guid); !pt.IsZero() {
			if pt.ParentProcessGUID == h.guid {
				return true
			}
		}
	}
	if sguid, ok := e.GetString(pathSysmonSourceProcessGUID); ok {
		if sguid == h.guid {
			return true
		}
		// search for parent in processTracker
		if pt := h.tracker.GetByGuid(sguid); !pt.IsZero() {
			if pt.ParentProcessGUID == h.guid {
				return true
			}
		}
	}
	return false
}

// Report generate a forensic ready report (meant to be dumped)
// this method is blocking as it runs commands and wait after those
func (h *HIDS) Report(light bool) (r Report) {
	r.StartTime = time.Now()

	// generate a report for running processes or those terminated still having one child or more
	// do this step first not to polute report with commands to run
	r.Processes = h.tracker.PS()

	// Modules ever loaded
	r.Modules = h.tracker.Modules()

	// Drivers loaded
	r.Drivers = h.tracker.Drivers

	// if this is a light report, we don't run the commands
	if !light {
		// run all the commands configured to include in the report
		r.Commands = h.config.Report.PrepareCommands()
		for i := range r.Commands {
			r.Commands[i].Run()
		}
	}

	r.StopTime = time.Now()
	return
}

// Run starts the WHIDS engine and waits channel listening is stopped
func (h *HIDS) Run() {
	// Running all the threads
	// Runs the forwarder
	h.forwarder.Run()

	// Running action manager
	h.actionHandler.Run()

	// Start the update routine
	log.Infof("Update routine running: %t", h.updateRoutine())
	// starting dump forwarding routine
	log.Infof("Dump forwarding routine running: %t", h.uploadRoutine())
	// running the command runner routine
	log.Infof("Command runner routine running: %t", h.commandRunnerRoutine())
	// start the archive cleanup routine (might create a new thread)
	log.Infof("Sysmon archived files cleanup routine running: %t", h.cleanArchivedRoutine())

	// Dry run don't do anything
	if h.DryRun {
		for _, trace := range h.config.EtwConfig.UnifiedTraces() {
			log.Infof("Dry run: would open trace %s", trace)
		}
		return
	}

	// Starting event provider
	h.eventProvider.Start()

	// start stats monitoring
	h.stats.Start()

	h.waitGroup.Add(1)
	go func() {
		defer h.waitGroup.Done()

		// Trying to raise thread priority
		if err := kernel32.SetCurrentThreadPriority(win32.THREAD_PRIORITY_ABOVE_NORMAL); err != nil {
			log.Errorf("Failed to raise IDS thread priority: %s", err)
		}

		for e := range h.eventProvider.Events {
			event := event.NewEdrEvent(e)

			if yes, eps := h.stats.HasPerfIssue(); yes {
				log.Warnf("Average event rate above limit of %.2f e/s in the last %s: %.2f e/s", h.stats.Threshold(), h.stats.Duration(), eps)

				if h.stats.HasCriticalPerfIssue() {
					log.Critical("Event throughput too high for too long, consider filtering out events")
				} else if crit := h.stats.CriticalEPS(); eps > crit {
					log.Criticalf("Event throughput above %.0fx the limit, if repeated consider filtering out events", eps/h.stats.Threshold())
				}
			}

			// Warning message in certain circumstances
			if h.config.EnableHooks && !h.flagProcTermEn && h.stats.Events() > 0 && int64(h.stats.Events())%1000 == 0 {
				log.Warn("Sysmon process termination events seem to be missing. WHIDS won't work as expected.")
			}

			h.RLock()

			// Runs pre detection hooks
			// putting this before next condition makes the processTracker registering
			// HIDS events and allows detecting ProcessAccess events from HIDS childs
			h.preHooks.RunHooksOn(h, event)

			// We skip if it is one of IDS event
			// we keep process termination event because it is used to control if process termination is enabled
			if h.IsHIDSEvent(event) && !isSysmonProcessTerminate(event) {
				if h.PrintAll {
					fmt.Println(utils.JsonString(event))
				}
				goto Continue
			}

			// if event is skipped we don't log it even with PrintAll
			if event.IsSkipped() {
				h.stats.Update(event)
				goto Continue
			}

			// if the event has matched at least one signature or is filtered
			if n, crit, filtered := h.Engine.MatchOrFilter(event); len(n) > 0 || filtered {
				switch {
				case crit >= h.config.CritTresh:
					if !h.PrintAll && !h.config.LogAll {
						h.forwarder.PipeEvent(event)
					}
					// Pipe the event to be sent to the forwarder
					// Run hooks post detection
					h.postHooks.RunHooksOn(h, event)
					h.stats.Update(event)
				case filtered && h.config.EnableFiltering && !h.PrintAll && !h.config.LogAll:
					//event.Del(&engine.GeneInfoPath)
					// we pipe filtered event
					h.forwarder.PipeEvent(event)
				}
			}

			// we queue event in action manager
			h.actionHandler.Queue(event)

			// Print everything
			if h.PrintAll {
				fmt.Println(utils.JsonString(event))
			}

			// We log all events
			if h.config.LogAll {
				h.forwarder.PipeEvent(event)
			}

			h.stats.Update(event)

		Continue:
			h.RUnlock()
		}
		log.Infof("HIDS main loop terminated")
	}()

	// Run bogus command so that at least one Process Terminate
	// is generated (used to check if process termination events are enabled)
	exec.Command(os.Args[0], "-h").Start()
}

// LogStats logs whids statistics
func (h *HIDS) LogStats() {
	log.Infof("Time Running: %s", h.stats.SinceStart())
	log.Infof("Count Event Scanned: %.0f", h.stats.Events())
	log.Infof("Average Event Rate: %.2f EPS", h.stats.EPS())
	log.Infof("Alerts Reported: %.0f", h.stats.Detections())
	log.Infof("Count Rules Used (loaded + generated): %d", h.Engine.Count())
}

// Stop stops the IDS
func (h *HIDS) Stop() {
	log.Infof("Stopping HIDS")
	// cancelling parent context
	h.cancel()
	// gently close forwarder needs to be done before
	// stop listening othewise we corrupt local logfiles
	// because of race condition
	log.Infof("Closing forwarder")
	h.forwarder.Close()

	// closing event provider
	log.Infof("Closing event provider")
	if err := h.eventProvider.Stop(); err != nil {
		log.Errorf("Error while closing event provider: %s", err)
	}

	// cleaning canary files
	if h.config.CanariesConfig.Enable {
		log.Infof("Cleaning canaries")
		h.config.CanariesConfig.Clean()
	}

	// updating autologger configuration
	log.Infof("Updating autologger configuration")
	if err := Autologger.Delete(); err != nil {
		log.Errorf("Failed to delete autologger:", err)
	}

	if err := h.config.EtwConfig.ConfigureAutologger(); err != nil {
		log.Errorf("Failed to update autologger configuration:", err)
	}

	log.Infof("HIDS stopped")
}

// Wait waits the IDS to finish
func (h *HIDS) Wait() {
	h.waitGroup.Wait()
}

// WaitWithTimeout waits the IDS to finish
func (h *HIDS) WaitWithTimeout(timeout time.Duration) {
	t := time.NewTimer(timeout)
	go func() {
		h.waitGroup.Wait()
		t.Stop()
	}()
	<-t.C
}
